# README - Natural Language Processing Assignment 5

## Overview
This project explores the implementation and fine-tuning of Transformer-based models and Large Language Models (LLMs) for various Natural Language Processing (NLP) tasks. The assignment involves sentiment classification using BERT, text generation with GPT-2, prompt engineering with Flan-T5, and an examination of biases in AI models.

## Assignment Tasks

### 1. Sentiment Classification with BERT (Fine-Tuning)
- **Dataset:** IMDb movie reviews
- **Objective:** Train a `bert-base-uncased` model to classify movie reviews as positive or negative.
- **Methodology:**
  1. Load and preprocess the IMDb dataset.
  2. Tokenize and split the data into training and evaluation sets.
  3. Fine-tune the BERT model using the Hugging Face `Trainer` API.
  4. Evaluate model performance and report accuracy metrics.
- **Results:** The model achieved an accuracy of **90%** on the test set.
- **Analysis:**
  - The results aligned with expectations given BERTâ€™s proven capabilities in text classification.
  - Performance may degrade when applied to datasets with different linguistic structures, such as book reviews.
- **Potential Improvements:**
  - Domain adaptation via additional fine-tuning on relevant datasets.
  - Optimization of hyperparameters such as learning rate and batch size.

### 2. Text Generation with GPT-2 (Fine-Tuning)
- **Objective:** Fine-tune two separate GPT-2 models for generating positive and negative movie reviews.
- **Methodology:**
  1. Train distinct GPT-2 models on positive and negative IMDb reviews.
  2. Utilize a structured hyperparameter tuning approach.
  3. Generate movie reviews using a standardized prompt ("The movie was").
- **Results:**
  - The generated reviews successfully reflected the intended sentiment.
  - Some limitations in coherence were observed due to the constrained dataset size.
- **Challenges and Considerations:**
  - Limited training data reduced generalization capability.
  - Increasing dataset size or incorporating transfer learning may improve output quality.

### 3. Prompt Engineering with Flan-T5
- **Objective:** Evaluate different prompting strategies for sentiment classification using Flan-T5.
- **Methodology:**
  - **Zero-shot prompting:** Directly instructing the model to classify sentiment without prior examples.
  - **Few-shot prompting:** Providing labeled examples before classification.
  - **Instruction-based prompting:** Using explicit instructions to guide classification.
- **Results:**
  - Few-shot prompting achieved the highest accuracy (96%).
  - Zero-shot and instruction-based prompting performed slightly lower at 94% accuracy.
- **Key Observations:**
  - Providing examples improved classification accuracy.
  - Excessive examples might lead to diminishing returns due to token limitations.
- **Potential Enhancements:**
  - Experimenting with more diverse prompts.
  - Refining instruction clarity to minimize ambiguity.

### 4. Bias Analysis in LLMs
- **Objective:** Assess and analyze biases in AI-generated outputs.
- **Findings:**
  - When prompted to generate an image of a female driver with a male passenger, the model frequently defaulted to a male driver.
  - NLP models also exhibited bias in role-based text generation.
- **Underlying Causes:**
  - Training data biases reinforce societal stereotypes.
  - Data distributions predominantly reflecting traditional gender roles.
- **Mitigation Strategies:**
  - Increasing dataset diversity.
  - Implementing fairness-aware training methodologies.

## Repository Contents
- `bert_classification_finetuning.py` - Script for fine-tuning BERT on sentiment classification.
- `gpt_generation_finetuning.py` - Script for training GPT-2 for sentiment-based text generation.
- `flan_t5_prompt_engineering.py` - Script for conducting prompt engineering experiments with Flan-T5.
- `generated_reviews.txt` - Output file containing reviews generated by the GPT-2 models.
- `flan_t5_imdb_results.txt` - Output file containing results from the Flan-T5 sentiment classification task.
- `hw5_report.pdf` - Comprehensive report detailing methodologies, findings, and analyses.

## Execution Instructions
### BERT Sentiment Classification
```sh
python bert_classification_finetuning.py <path/to/imdb_subset>
```

### GPT-2 Text Generation
```sh
python gpt_generation_finetuning.py <path/to/imdb_subset> <path/to/generated_reviews.txt> <path/to/saved_models>
```

### Flan-T5 Prompt Engineering
```sh
python flan_t5_prompt_engineering.py <path/to/imdb_subset> <path/to/flan_t5_imdb_results.txt>
```

## Conclusion
This assignment demonstrates the effectiveness of Transformer-based models in NLP applications, highlighting their strengths and potential areas for improvement. While these models exhibit strong performance in classification and text generation, bias remains a critical challenge that necessitates continued research and refinement to ensure ethical and fair AI deployment.

